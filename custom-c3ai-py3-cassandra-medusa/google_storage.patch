diff --git a/medusa/storage/google_storage.py b/medusa/storage/google_storage.py
index 5ea6df2..ad659d2 100644
--- a/medusa/storage/google_storage.py
+++ b/medusa/storage/google_storage.py
@@ -29,7 +29,6 @@ from gcloud.aio.storage import Storage
 
 from medusa.storage.abstract_storage import AbstractStorage, AbstractBlob, ManifestObject, ObjectDoesNotExistError
 
-
 DOWNLOAD_STREAM_CONSUMPTION_CHUNK_SIZE = 1024 * 1024 * 5
 GOOGLE_MAX_FILES_PER_CHUNK = 64
 MAX_UP_DOWN_LOAD_RETRIES = 5
@@ -77,7 +76,7 @@ class GoogleStorage(AbstractStorage):
             AbstractBlob(
                 o['name'],
                 int(o['size']),
-                o['md5Hash'],
+                _get_blob_hash(o),
                 # datetime comes as a string like 2023-08-31T14:23:24.957Z
                 datetime.datetime.strptime(o['timeCreated'], '%Y-%m-%dT%H:%M:%S.%fZ')
             )
@@ -128,7 +127,7 @@ class GoogleStorage(AbstractStorage):
             force_resumable_upload=True,
             timeout=-1,
         )
-        return AbstractBlob(resp['name'], int(resp['size']), resp['md5Hash'], resp['timeCreated'])
+        return AbstractBlob(resp['name'], int(resp['size']), _get_blob_hash(resp), resp['timeCreated'])
 
     @retry(stop_max_attempt_number=MAX_UP_DOWN_LOAD_RETRIES, wait_fixed=5000)
     async def _download_blob(self, src: str, dest: str):
@@ -174,7 +173,7 @@ class GoogleStorage(AbstractStorage):
         return AbstractBlob(
             blob['name'],
             int(blob['size']),
-            blob['md5Hash'],
+            _get_blob_hash(blob),
             # datetime comes as a string like 2023-08-31T14:23:24.957Z
             datetime.datetime.strptime(blob['timeCreated'], '%Y-%m-%dT%H:%M:%S.%fZ')
         )
@@ -215,7 +214,7 @@ class GoogleStorage(AbstractStorage):
                     force_resumable_upload=True,
                     timeout=-1,
                 )
-        mo = ManifestObject(resp['name'], int(resp['size']), resp['md5Hash'])
+        mo = ManifestObject(resp['name'], int(resp['size']), _get_blob_hash(resp))
         return mo
 
     async def _get_object(self, object_key: str) -> AbstractBlob:
@@ -299,3 +298,15 @@ def _group_by_parent(paths):
     by_parent = itertools.groupby(paths, lambda p: Path(p).parent.name)
     for parent, files in by_parent:
         yield parent, list(files)
+
+
+# When the GCS Bucket is encrypted with customer-managed key (which is what we do at c3),
+# GCS list will not include the field `md5Hash` in the response.
+# Ref: https://console.cloud.google.com/support/cases/detail/v2/47689283?project=gketestops1
+#
+# Therefore, we use the etag field instead of md5Hash.
+def _get_blob_hash(blob):
+    if 'etag' in blob:
+        return blob['etag']
+
+    return 'no-md5-hash'
